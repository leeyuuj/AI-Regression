# 선형 회귀 (Linear Regression)

> 종속 변수 : $y$ </br>
> 독립 변수 (또는 설명 변수) : $x$

![Normdist_regression](https://user-images.githubusercontent.com/109254161/183783366-fcead578-21c8-4d4c-9586-4996b2fc9a57.png)

## 정의 

$y$와 1개 이상의 $x$의 선형 상관 관계를 모델링하는 회귀분석 기법 [출처 : wikipedia]

### 단순 선형 회귀 분석 (Simple Linear Regression Analysis)
>하나의 $x$ 값으로 $y$ 값을 설명할 수 있는 경우를 단순 선형 회귀(simple linear regression)라고 한다.

$y = \beta_0 + \beta_1x $

* 가중치 : $\beta_1$ (weight)
* 편향 : $\beta_0$

![스크린샷_2017-11-15_오전_12 33 47](https://user-images.githubusercontent.com/109254161/183783423-9aff100c-6140-4b5a-8e0d-3ba63fce0aab.png)

[출처 : https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=anthouse28&logNo=221149064073]

"데이터를 가장 잘 설명하는"이라는 의미 : "실제값과 예측차이가 작은"
> 최적의 회귀모델을 만든다 => 잔차(오차값)합이 최소가 되는 모델을 만든다</br>
> 선형 회귀 모델은 최적의 $w$와 $b$를 찾기 위해 비용함수 MSE를 활용하여 경사하강법을 수행한다.

## 최소제곱법 (Least squares)
* 실제 $y$ 값과 예측 $y$값의 차이, 즉 데이터의 잔차 합이 최소가 되는 모델
* 잔차의 제곱합(RSS; Residual Sum of Squares)을 최소화하는 계수 찾기

$RSS = \sum_{i=0}^n e_i^2 = e_1^2 + e_2^2 + \dots + e_n^2 $

#### 오차항에 대한 가정
오차항은 모수이므로 우리는 그 값을 알수없다.</br>
때문에 잔차항 e로 가설을 검정하여, 다음의 가설이 성립하면 회귀 분석을 시행한다.

1. 오차들의 평균 = 0
2. 등분산성 : 오차들은 같은 정도로 퍼져있어야한다.
3. 독립성 : 오차항끼리는 독립
4. 정규성 : 오차들은 평균 0을 중심으로 정규분포를 이룬다.

### 다중 선형 회귀 (Multiple Linear Regression)

> 2개 이상의 $x1, x2, x3$ 등의 값을 사용하는 경우를 다중 선형 회귀(multiple linear regression)라고 한다.</br>

$y = \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + b$

현실에서 일어나는 대부분의 일은 여러 개의 독립 변수를 가질 수밖에 없으므로 다중 선형 회귀가 많이 사용되며, 일반적으로 선형 회귀는 다중 선형 회귀를 나타낸다.

## 선형 회귀의 장점
1. 복잡한 모델을 쉬운 계산들로 분해할 수 있다.

2. 모델의 해석이 쉽고 직관적이다.

3. 복잡한 패턴을 요구하지 않는 데이터에서 견고한 추정이 가능하다.


## 가설 함수 
* 데이터 속에서 가장 적합한 최적선을 찾기위해 다양한 값들을 넣어서 시도해봐야하는데, 이렇게 만들어진 함수를 가설함수라고 한다.

$h_\theta(x)= \theta_0+\theta_1x_1+\theta_2x_2+...$

선형회귀에서는 적절한 $\theta$값을 찾으면 된다.

### 평균 제곱 오차(MSE)

평균 제곱 오차는 각 데이터와 가설 함수 사이에 나타나는 오차를 다 구하여 오차값들을 모두 제곱한다. </br>
(제곱하는 이유는 데이터가 음수일 수 있기 때문에, 이들을 양수로 통일하기 위해서, 오차를 더 부각시키기 위해)

$ \frac{1} {m} \sum_{i=1}^{m}(h_\theta(x^i)-y^i)^2$

### 손실 함수
> 가설함수를 평가하기 위한 함수
> 손실 함수의 결과가 작을수록 가설함수의 손실이 적다는 것이기에 좋은 가설 함수라고 평가할수 있다. 

$ J(\theta)= \frac{1}{2m} \sum_{i=1}{m}(h_\theta(x^i)-y^i)^2 $

# 비선형 회귀 (Nonlinear Regression)

> 어떤 형태로 위의 식들과 같이 변환해도 $coefficient$(계수)를 선형식으로 표현하기 어려운 비선형 관계를 나타내는 회귀 분석

> 대표적 활성화 함수 : 시그모이드 함수, 탄젠트 함수, ReLU 함수
 * 출력을 0과 1로 이진값만 반환

$ \sigma=\frac{1}{1+e^-x}$

![nonlinear](https://user-images.githubusercontent.com/109254161/183783465-b88687d1-b290-4dac-a919-9d7281309bb0.jpeg)

[출처 : https://reniew.github.io/12/]

* 일정 구간은 선형성을 띄지만 일정구간은 선형적 성격을 띄지 않는다.

## 장점 및 특징

* 선형모델보다 정확하고 유연성을 지니고있어 둘 이상의 변수간의 복잡한 패턴을 갖는 데이터도 예측이 가능하고 다양한 곡선을 수용할수있다.</br>
* 충분히 많은 데이터를 갖고 있어서 variance error를 충분히 줄일수 있고 예측 자체가 목적이면 사용할만한 도구이다.
* 정교한 예측의 경우 뉴럴 네트워크와 같은 비선형 모델을 사용중
* 신경 회로망 가운데 가장 많이 사용되는 multi-layer perceptron(다층 퍼셉트론)
* 활성화 함수가 시그모이드 함수인경우
→ Logistic Regression
* 딥러닝 알고리즘(RNN, CNN 등)에 사용하고 있다.


### 시그모이드 함수의 단점
* 입력값이 아무리 크더라도, 출력되는 값의 범위가 매우 좁기 때문에 경사하강법 수행시에 범위가 너무 좁아, 0에 수렴하는 기울기 소실이 발생할수 있다.

## 탄젠트 함수

$tanh(x) = \sigma(2x)-1 $ </br>
$tanh(x) = \frac {e^x-e^(-x)}{e^x+e^(-x)} $

![tanh](https://user-images.githubusercontent.com/109254161/183783501-ed4532f9-bcc7-4637-92c8-74dfb27479cc.png)

[출처 : https://reniew.github.io/12/]

### 탄젠트 함수의 단점

* 미분함수에 대해 일정값 이상 커질시 미분값이 소실되는 기울기 소실 문제는 여전히 남아있다.

## ReLU 함수 (Rectified Linear Unit)

$f(x) = max(0, x) $


![ReLU](https://user-images.githubusercontent.com/109254161/183783535-f3e4136d-fc57-45fc-8403-a003df1b1761.png)


[출처 : https://reniew.github.io/12/]


### ReLU함수의 특징
* $x > 0$ 이면 기울기가 1인 직선이고, $x < 0$ 이면 함수값이 $0$이된다.
* sigmoid, tanh 함수와 비교시 학습이 훨씬 빨라진다.
* 연산 비용이 크지않고, 구현이 매우 간단하다.
* $x < 0$ 인 값들에 대해서는 기울기가 $0$이기 때문에 뉴런이 죽을 수 있는 단점이 존재한다.

# 선형 회귀와 비선형 회귀의 차이점 


## 모수의 해석</br>

* 선형회귀모형에서 회귀계수 : 설명변수의 변화량에 따른 반응변수의 평균변화량</br>

* 비선형회귀모형에서 회귀계수 : 각 모수가 특정한 의미를 가질수 있다.

## 비교
* 선형은 직선 fit
* 비선형은 곡선 fit

## 퍼셉트론 구조
* 선형 회귀
> 출력 노드 1개, 중간층이 없는 망

* 비선형 회귀 
> 히든 노드 : sigmoid 함수</br>
출력 노드 : linear (또는 identity)

---

# 회귀 (regression)

## 역사

회귀의 원래 의미는 옛날 상태로 돌아가는 것을 의미한다. 영국의 유전학자 프랜시스 골턴은 부모의 키와 아이들의 키 사이의 연관 관계를 연구하면서 부모와 자녀의 키사이에는 선형적인 관계가 있고 키가 커지거나 작아지는 것보다는 전체 키 평균으로 돌아가려는 경향이 있다는 가설을 세웠으며 이를 분석하는 방법을 "회귀분석"이라고 하였다. 이러한 경험적 연구 이후, 칼 피어슨은 아버지와 아들의 키를 조사한 결과를 바탕으로 함수 관계를 도출하여 회귀분석 이론을 수학적으로 정립하였다.

[출처 : 위키피디아]

## 정의
* 관찰된 연속형 변수들에 대해 두 변수 사이의 모형을 구한 뒤 적합도를 측정해내는 분석 방법

---

# 인공지능 (AI)

## 역사

상당수 인공지능 연구의 (본래) 목적은 심리학에 대한 실험적인 접근이었고, 언어 지능(linguistic intelligence)이 무엇인지를 밝혀내는 것이 주목표였다

인공지능 연구에 바탕을 둔 실질적인 작업이 결실을 거둠에 따라, 인공지능을 지지하는 사람들은 인공지능의 업적을 깎아내리기 위해 인공지능에 반대하는 사람들이 예전에는 '지능적'인 일이라고 주장하던 컴퓨터 체스나 바둑, 음성인식 등과 같은 작업에 대해 말을 바꾸고 있다고 비난하였다. 그들은 이와 같이 연구 목표를 옮기는 작업은 '지능'을 '인간은 할 수 있지만, 기계는 할 수 없는 어떤 것'으로 정의하는 역할을 한다고 지적하였다.

당시 신경학의 최신 연구는 실제 뇌가 뉴런으로 이루어진 전기적인 네트워크라고 보았다. 위너가 인공두뇌학을 전기적 네트워크의 제어와 안정화로 묘사했으며, 섀넌의 정보 과학은 디지털 신호로 묘사했다. 또 튜링의 계산 이론은 어떤 형태의 계산도 디지털로 나타낼 수 있음을 보였다. 이런 여러 밀접한 연관에서, 인공두뇌의 전자적 구축에 대한 아이디어가 나온 것이다.

## 튜링 테스트

## 게임 인공지능

## 다트머스 컨퍼런스 1956년 
 * AI 탄생

## 자연어 처리</br>
 '의미 망'은 개념을 다른 개념들 사이의 노드와 링크 관계로 나타낸다.

## AI의 첫번째 암흑기(1974-1980)
70년대에 이르자, AI는 비판의 대상이 되었고 재정적 위기가 닥쳤다. AI 연구가들은 그들의 눈앞에 있는 복잡한 문제를 해결하는데 실패했다. 연구가들의 엄청난 낙관론은 연구에 대한 기대를 매우 높여놓았고, 그들이 약속했던 결과를 보여주지 못하자, AI에 대한 자금 투자는 사라져버렸다. 동시에, Connectionism 또는 뉴럴망은 지난 10년동안 마빈 민스키의 퍼셉트론(시각과 뇌의 기능을 모델화한 학습 기계)에 대한 파괴적인 비판에 의해 완전히 중지되었다.그러나 70년대 후반의 AI에 대한 좋지 않은 대중의 인식에도 불구하고, 논리 프로그래밍, 상징 추론과 많은 여러 영역에서의 새로운 아이디어가 나타났다

### 문제
* 컴퓨터 능력의 한계</br>
메모리 또는 처리속도가 충분하지 않았다.

* 폭발적인 조합수와 비용이성</br>
최적의 해답을 찾는데 컴퓨터의 많은 시간이 요구되었다.

* 상징적 지식과 추론</br>
방대한 양의 정보를 필요로 한다.

## 신경망 이론의 복귀
1982년 , 물리학자 John Hopfield는 완벽한 새로운 길에서 정보를 프로세스하고 배울 수 있는 신경망의 형태를 증명해냈다. 이 시기에, David Rumelhart는“역전파”라고 불리는 신경망을 개선하기 위한 새로운 방법을 알리고 있었다. 이러한 두 가지 발견은 1970년 이후 버려진 신경망 이론이라는 분야를 복구시켰다. 새로운 분야는 1986년 분산 병렬처리의 형태로부터 영감을 받았고 이와 같은 형태로 통일되었다. 2권 분량의 논문 집합은 Rumelhart와 물리학자인 James McClelland에 의해 편집되었다. 신경망은 1990년대에 광학 문자 인식 및 음성 인식과 같은 프로그램의 구동 엔진으로 사용되며 상업적으로 성공했다.

## 현재의 AI
